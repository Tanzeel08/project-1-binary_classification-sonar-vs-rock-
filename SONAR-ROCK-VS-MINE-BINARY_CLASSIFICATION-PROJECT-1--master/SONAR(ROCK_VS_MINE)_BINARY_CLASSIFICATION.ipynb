{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy \n",
    "import pandas \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y=encoder.transform(Y)\n",
    "mine=1\n",
    "rock=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0       1       2       3       4       5       6       7       8   \\\n",
      "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
      "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
      "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
      "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
      "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
      "5    0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n",
      "6    0.0317  0.0956  0.1321  0.1408  0.1674  0.1710  0.0731  0.1401  0.2083   \n",
      "7    0.0519  0.0548  0.0842  0.0319  0.1158  0.0922  0.1027  0.0613  0.1465   \n",
      "8    0.0223  0.0375  0.0484  0.0475  0.0647  0.0591  0.0753  0.0098  0.0684   \n",
      "9    0.0164  0.0173  0.0347  0.0070  0.0187  0.0671  0.1056  0.0697  0.0962   \n",
      "10   0.0039  0.0063  0.0152  0.0336  0.0310  0.0284  0.0396  0.0272  0.0323   \n",
      "11   0.0123  0.0309  0.0169  0.0313  0.0358  0.0102  0.0182  0.0579  0.1122   \n",
      "12   0.0079  0.0086  0.0055  0.0250  0.0344  0.0546  0.0528  0.0958  0.1009   \n",
      "13   0.0090  0.0062  0.0253  0.0489  0.1197  0.1589  0.1392  0.0987  0.0955   \n",
      "14   0.0124  0.0433  0.0604  0.0449  0.0597  0.0355  0.0531  0.0343  0.1052   \n",
      "15   0.0298  0.0615  0.0650  0.0921  0.1615  0.2294  0.2176  0.2033  0.1459   \n",
      "16   0.0352  0.0116  0.0191  0.0469  0.0737  0.1185  0.1683  0.1541  0.1466   \n",
      "17   0.0192  0.0607  0.0378  0.0774  0.1388  0.0809  0.0568  0.0219  0.1037   \n",
      "18   0.0270  0.0092  0.0145  0.0278  0.0412  0.0757  0.1026  0.1138  0.0794   \n",
      "19   0.0126  0.0149  0.0641  0.1732  0.2565  0.2559  0.2947  0.4110  0.4983   \n",
      "20   0.0473  0.0509  0.0819  0.1252  0.1783  0.3070  0.3008  0.2362  0.3830   \n",
      "21   0.0664  0.0575  0.0842  0.0372  0.0458  0.0771  0.0771  0.1130  0.2353   \n",
      "22   0.0099  0.0484  0.0299  0.0297  0.0652  0.1077  0.2363  0.2385  0.0075   \n",
      "23   0.0115  0.0150  0.0136  0.0076  0.0211  0.1058  0.1023  0.0440  0.0931   \n",
      "24   0.0293  0.0644  0.0390  0.0173  0.0476  0.0816  0.0993  0.0315  0.0736   \n",
      "25   0.0201  0.0026  0.0138  0.0062  0.0133  0.0151  0.0541  0.0210  0.0505   \n",
      "26   0.0151  0.0320  0.0599  0.1050  0.1163  0.1734  0.1679  0.1119  0.0889   \n",
      "27   0.0177  0.0300  0.0288  0.0394  0.0630  0.0526  0.0688  0.0633  0.0624   \n",
      "28   0.0100  0.0275  0.0190  0.0371  0.0416  0.0201  0.0314  0.0651  0.1896   \n",
      "29   0.0189  0.0308  0.0197  0.0622  0.0080  0.0789  0.1440  0.1451  0.1789   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "178  0.0197  0.0394  0.0384  0.0076  0.0251  0.0629  0.0747  0.0578  0.1357   \n",
      "179  0.0394  0.0420  0.0446  0.0551  0.0597  0.1416  0.0956  0.0802  0.1618   \n",
      "180  0.0310  0.0221  0.0433  0.0191  0.0964  0.1827  0.1106  0.1702  0.2804   \n",
      "181  0.0423  0.0321  0.0709  0.0108  0.1070  0.0973  0.0961  0.1323  0.2462   \n",
      "182  0.0095  0.0308  0.0539  0.0411  0.0613  0.1039  0.1016  0.1394  0.2592   \n",
      "183  0.0096  0.0404  0.0682  0.0688  0.0887  0.0932  0.0955  0.2140  0.2546   \n",
      "184  0.0269  0.0383  0.0505  0.0707  0.1313  0.2103  0.2263  0.2524  0.3595   \n",
      "185  0.0340  0.0625  0.0381  0.0257  0.0441  0.1027  0.1287  0.1850  0.2647   \n",
      "186  0.0209  0.0191  0.0411  0.0321  0.0698  0.1579  0.1438  0.1402  0.3048   \n",
      "187  0.0368  0.0279  0.0103  0.0566  0.0759  0.0679  0.0970  0.1473  0.2164   \n",
      "188  0.0089  0.0274  0.0248  0.0237  0.0224  0.0845  0.1488  0.1224  0.1569   \n",
      "189  0.0158  0.0239  0.0150  0.0494  0.0988  0.1425  0.1463  0.1219  0.1697   \n",
      "190  0.0156  0.0210  0.0282  0.0596  0.0462  0.0779  0.1365  0.0780  0.1038   \n",
      "191  0.0315  0.0252  0.0167  0.0479  0.0902  0.1057  0.1024  0.1209  0.1241   \n",
      "192  0.0056  0.0267  0.0221  0.0561  0.0936  0.1146  0.0706  0.0996  0.1673   \n",
      "193  0.0203  0.0121  0.0380  0.0128  0.0537  0.0874  0.1021  0.0852  0.1136   \n",
      "194  0.0392  0.0108  0.0267  0.0257  0.0410  0.0491  0.1053  0.1690  0.2105   \n",
      "195  0.0129  0.0141  0.0309  0.0375  0.0767  0.0787  0.0662  0.1108  0.1777   \n",
      "196  0.0050  0.0017  0.0270  0.0450  0.0958  0.0830  0.0879  0.1220  0.1977   \n",
      "197  0.0366  0.0421  0.0504  0.0250  0.0596  0.0252  0.0958  0.0991  0.1419   \n",
      "198  0.0238  0.0318  0.0422  0.0399  0.0788  0.0766  0.0881  0.1143  0.1594   \n",
      "199  0.0116  0.0744  0.0367  0.0225  0.0076  0.0545  0.1110  0.1069  0.1708   \n",
      "200  0.0131  0.0387  0.0329  0.0078  0.0721  0.1341  0.1626  0.1902  0.2610   \n",
      "201  0.0335  0.0258  0.0398  0.0570  0.0529  0.1091  0.1709  0.1684  0.1865   \n",
      "202  0.0272  0.0378  0.0488  0.0848  0.1127  0.1103  0.1349  0.2337  0.3113   \n",
      "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
      "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
      "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
      "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
      "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
      "\n",
      "         9   ...      51      52      53      54      55      56      57  \\\n",
      "0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
      "1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
      "2    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
      "3    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
      "4    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
      "5    0.3039  ...  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057  0.0027   \n",
      "6    0.3513  ...  0.0201  0.0248  0.0131  0.0070  0.0138  0.0092  0.0143   \n",
      "7    0.2838  ...  0.0081  0.0120  0.0045  0.0121  0.0097  0.0085  0.0047   \n",
      "8    0.1487  ...  0.0145  0.0128  0.0145  0.0058  0.0049  0.0065  0.0093   \n",
      "9    0.0251  ...  0.0090  0.0223  0.0179  0.0084  0.0068  0.0032  0.0035   \n",
      "10   0.0452  ...  0.0062  0.0120  0.0052  0.0056  0.0093  0.0042  0.0003   \n",
      "11   0.0835  ...  0.0133  0.0265  0.0224  0.0074  0.0118  0.0026  0.0092   \n",
      "12   0.1240  ...  0.0176  0.0127  0.0088  0.0098  0.0019  0.0059  0.0058   \n",
      "13   0.1895  ...  0.0059  0.0095  0.0194  0.0080  0.0152  0.0158  0.0053   \n",
      "14   0.2120  ...  0.0083  0.0057  0.0174  0.0188  0.0054  0.0114  0.0196   \n",
      "15   0.0852  ...  0.0031  0.0153  0.0071  0.0212  0.0076  0.0152  0.0049   \n",
      "16   0.2912  ...  0.0346  0.0158  0.0154  0.0109  0.0048  0.0095  0.0015   \n",
      "17   0.1186  ...  0.0331  0.0131  0.0120  0.0108  0.0024  0.0045  0.0037   \n",
      "18   0.1520  ...  0.0084  0.0010  0.0018  0.0068  0.0039  0.0120  0.0132   \n",
      "19   0.5920  ...  0.0092  0.0035  0.0098  0.0121  0.0006  0.0181  0.0094   \n",
      "20   0.3759  ...  0.0193  0.0118  0.0064  0.0042  0.0054  0.0049  0.0082   \n",
      "21   0.1838  ...  0.0141  0.0190  0.0043  0.0036  0.0026  0.0024  0.0162   \n",
      "22   0.1882  ...  0.0173  0.0149  0.0115  0.0202  0.0139  0.0029  0.0160   \n",
      "23   0.0734  ...  0.0091  0.0016  0.0084  0.0064  0.0026  0.0029  0.0037   \n",
      "24   0.0860  ...  0.0035  0.0052  0.0083  0.0078  0.0075  0.0105  0.0160   \n",
      "25   0.1097  ...  0.0108  0.0070  0.0063  0.0030  0.0011  0.0007  0.0024   \n",
      "26   0.1205  ...  0.0061  0.0015  0.0084  0.0128  0.0054  0.0011  0.0019   \n",
      "27   0.0613  ...  0.0102  0.0122  0.0044  0.0075  0.0124  0.0099  0.0057   \n",
      "28   0.2668  ...  0.0088  0.0104  0.0036  0.0088  0.0047  0.0117  0.0020   \n",
      "29   0.2522  ...  0.0038  0.0096  0.0142  0.0190  0.0140  0.0099  0.0092   \n",
      "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "178  0.1695  ...  0.0134  0.0097  0.0042  0.0058  0.0072  0.0041  0.0045   \n",
      "179  0.2558  ...  0.0146  0.0040  0.0114  0.0032  0.0062  0.0101  0.0068   \n",
      "180  0.4432  ...  0.0204  0.0059  0.0053  0.0079  0.0037  0.0015  0.0056   \n",
      "181  0.2696  ...  0.0176  0.0035  0.0093  0.0121  0.0075  0.0056  0.0021   \n",
      "182  0.3745  ...  0.0181  0.0019  0.0102  0.0133  0.0040  0.0042  0.0030   \n",
      "183  0.2952  ...  0.0237  0.0078  0.0144  0.0170  0.0012  0.0109  0.0036   \n",
      "184  0.5915  ...  0.0167  0.0199  0.0145  0.0081  0.0045  0.0043  0.0027   \n",
      "185  0.4117  ...  0.0141  0.0019  0.0067  0.0099  0.0042  0.0057  0.0051   \n",
      "186  0.3914  ...  0.0078  0.0201  0.0104  0.0039  0.0031  0.0062  0.0087   \n",
      "187  0.2544  ...  0.0105  0.0024  0.0018  0.0057  0.0092  0.0009  0.0086   \n",
      "188  0.2119  ...  0.0096  0.0103  0.0093  0.0025  0.0044  0.0021  0.0069   \n",
      "189  0.1923  ...  0.0121  0.0108  0.0057  0.0028  0.0079  0.0034  0.0046   \n",
      "190  0.1567  ...  0.0150  0.0060  0.0082  0.0091  0.0038  0.0056  0.0056   \n",
      "191  0.1533  ...  0.0108  0.0062  0.0044  0.0072  0.0007  0.0054  0.0035   \n",
      "192  0.1859  ...  0.0072  0.0055  0.0074  0.0068  0.0084  0.0037  0.0024   \n",
      "193  0.1747  ...  0.0134  0.0094  0.0047  0.0045  0.0042  0.0028  0.0036   \n",
      "194  0.2471  ...  0.0083  0.0080  0.0026  0.0079  0.0042  0.0071  0.0044   \n",
      "195  0.2245  ...  0.0124  0.0093  0.0072  0.0019  0.0027  0.0054  0.0017   \n",
      "196  0.2282  ...  0.0165  0.0056  0.0010  0.0027  0.0062  0.0024  0.0063   \n",
      "197  0.1847  ...  0.0132  0.0027  0.0022  0.0059  0.0016  0.0025  0.0017   \n",
      "198  0.2048  ...  0.0096  0.0071  0.0084  0.0038  0.0026  0.0028  0.0013   \n",
      "199  0.2271  ...  0.0141  0.0103  0.0100  0.0034  0.0026  0.0037  0.0044   \n",
      "200  0.3193  ...  0.0150  0.0076  0.0032  0.0037  0.0071  0.0040  0.0009   \n",
      "201  0.2660  ...  0.0120  0.0039  0.0053  0.0062  0.0046  0.0045  0.0022   \n",
      "202  0.3997  ...  0.0091  0.0045  0.0043  0.0043  0.0098  0.0054  0.0051   \n",
      "203  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
      "204  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
      "205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
      "206  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
      "207  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
      "\n",
      "         58      59  60  \n",
      "0    0.0090  0.0032   R  \n",
      "1    0.0052  0.0044   R  \n",
      "2    0.0095  0.0078   R  \n",
      "3    0.0040  0.0117   R  \n",
      "4    0.0107  0.0094   R  \n",
      "5    0.0051  0.0062   R  \n",
      "6    0.0036  0.0103   R  \n",
      "7    0.0048  0.0053   R  \n",
      "8    0.0059  0.0022   R  \n",
      "9    0.0056  0.0040   R  \n",
      "10   0.0053  0.0036   R  \n",
      "11   0.0009  0.0044   R  \n",
      "12   0.0059  0.0032   R  \n",
      "13   0.0189  0.0102   R  \n",
      "14   0.0147  0.0062   R  \n",
      "15   0.0200  0.0073   R  \n",
      "16   0.0073  0.0067   R  \n",
      "17   0.0112  0.0075   R  \n",
      "18   0.0070  0.0088   R  \n",
      "19   0.0116  0.0063   R  \n",
      "20   0.0028  0.0027   R  \n",
      "21   0.0109  0.0079   R  \n",
      "22   0.0106  0.0134   R  \n",
      "23   0.0070  0.0041   R  \n",
      "24   0.0095  0.0011   R  \n",
      "25   0.0057  0.0044   R  \n",
      "26   0.0023  0.0062   R  \n",
      "27   0.0032  0.0019   R  \n",
      "28   0.0091  0.0058   R  \n",
      "29   0.0052  0.0075   R  \n",
      "..      ...     ...  ..  \n",
      "178  0.0047  0.0054   M  \n",
      "179  0.0053  0.0087   M  \n",
      "180  0.0067  0.0054   M  \n",
      "181  0.0043  0.0017   M  \n",
      "182  0.0031  0.0033   M  \n",
      "183  0.0043  0.0018   M  \n",
      "184  0.0055  0.0057   M  \n",
      "185  0.0033  0.0058   M  \n",
      "186  0.0070  0.0042   M  \n",
      "187  0.0110  0.0052   M  \n",
      "188  0.0060  0.0018   M  \n",
      "189  0.0022  0.0021   M  \n",
      "190  0.0048  0.0024   M  \n",
      "191  0.0001  0.0055   M  \n",
      "192  0.0034  0.0007   M  \n",
      "193  0.0013  0.0016   M  \n",
      "194  0.0022  0.0014   M  \n",
      "195  0.0024  0.0029   M  \n",
      "196  0.0017  0.0028   M  \n",
      "197  0.0027  0.0027   M  \n",
      "198  0.0035  0.0060   M  \n",
      "199  0.0057  0.0035   M  \n",
      "200  0.0015  0.0085   M  \n",
      "201  0.0005  0.0031   M  \n",
      "202  0.0065  0.0103   M  \n",
      "203  0.0193  0.0157   M  \n",
      "204  0.0062  0.0067   M  \n",
      "205  0.0077  0.0031   M  \n",
      "206  0.0036  0.0048   M  \n",
      "207  0.0061  0.0115   M  \n",
      "\n",
      "[208 rows x 61 columns]\n",
      "[[0.02   0.0371 0.0428 ... 0.0084 0.009  0.0032]\n",
      " [0.0453 0.0523 0.0843 ... 0.0049 0.0052 0.0044]\n",
      " [0.0262 0.0582 0.1099 ... 0.0164 0.0095 0.0078]\n",
      " ...\n",
      " [0.0522 0.0437 0.018  ... 0.0138 0.0077 0.0031]\n",
      " [0.0303 0.0353 0.049  ... 0.0079 0.0036 0.0048]\n",
      " [0.026  0.0363 0.0136 ... 0.0036 0.0061 0.0115]]\n",
      "********************************\n",
      "['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M'\n",
      " 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M' 'M']\n"
     ]
    }
   ],
   "source": [
    "print(dataframe)\n",
    "print(X)\n",
    "print(\"********************************\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-f2UxxRBM6X5"
   },
   "source": [
    "**Baseline Neural Network Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "def create_baseline():\n",
    "  # create model, write code below\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    model=tf.keras.Sequential()\n",
    "  #input layer\n",
    "    model.add(Dense(60,activation='relu',input_shape=(60,)))\n",
    "  #output layer\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "\t# Compile model, write code below\n",
    "    model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 83.68% (4.89%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X,encoded_Y, cv=kfold)\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2dxlj4uUwas"
   },
   "source": [
    "**Re-Run The Baseline Model With Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "def create_baseline():\n",
    "  # create model, write code below\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    model=tf.keras.Sequential()\n",
    "  #input layer\n",
    "    model.add(Dense(60,activation='relu',input_shape=(60,)))\n",
    "  #output layer\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "\t# Compile model, write code below\n",
    "    model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 85.52% (6.95%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate baseline model with standardized dataset\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X,encoded_Y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "npw2YpI9YrhS"
   },
   "source": [
    "# Tuning Layers and Number of Neurons in The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMgJAYdJZCqK"
   },
   "source": [
    "**Evaluate a Smaller Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller model\n",
    "def create_smaller():\n",
    "  # create model\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    model=tf.keras.Sequential()\n",
    "  #input layer\n",
    "    model.add(Dense(30,activation='relu',input_shape=(60,)))\n",
    "  #output layer\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "\t\n",
    "    model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\t# Compile model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller: 85.54% (6.19%)\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86363637 0.95238096 0.76190478 0.85714287 0.90476191 0.80952382\n",
      " 0.95238096 0.75       0.80000001 0.89999998]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "be5X-py_axIX"
   },
   "source": [
    "**Evaluate a Larger Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger model\n",
    "def create_larger():\n",
    "   # create model\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    model=tf.keras.Sequential()\n",
    "  #input layer\n",
    "    model.add(Dense(60,activation='relu',input_shape=(60,)))\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "  #output layer\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "\t\n",
    "    model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\t# Compile model\n",
    "  \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 84.63% (7.04%)\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9QGhHlos33u"
   },
   "source": [
    "**Really Scaling up: developing a model that overfits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger model\n",
    "def create_larger():\n",
    "   # create model\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    model=tf.keras.Sequential()\n",
    "  #input layer\n",
    "    model.add(Dense(80,activation='relu',input_shape=(60,)))\n",
    "    model.add(Dense(60,activation='relu'))\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "  #output layer\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "\t\n",
    "    model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\t# Compile model\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 84.61% (7.35%)\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=800, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evffm3JryrhU"
   },
   "source": [
    "**Tuning the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger model\n",
    "def create_Tunning_model():\n",
    "   # create model\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    model=tf.keras.Sequential()\n",
    "  #input layer\n",
    "    model.add(Dense(80,activation='relu',input_shape=(60,)))\n",
    "    model.add(Dense(60,activation='relu'))\n",
    "    model.add(Dense(30,activation='relu'))\n",
    "\n",
    "            #output layer\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "\t\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\t# Compile model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunnig: 84.59% (7.13%)\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_Tunning_model, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Tunnig: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A78AHYzvzfMI"
   },
   "source": [
    "**Rewriting the code using the Keras Functional API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 88.62% (7.20%)\n"
     ]
    }
   ],
   "source": [
    "#import keras \n",
    "#from keras import models\n",
    "#from keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "inputs=Input(shape=(60,))\n",
    "x=Dense(60,activation='relu')(inputs)\n",
    "outputs=Dense(1,activation='sigmoid')(x)\n",
    "model=Model(inputs=inputs,outputs=outputs)\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "history=model.fit(X, encoded_Y,epochs=100, batch_size=5, verbose=0)\n",
    "#print(\"Tunnig: %.2f%% (%.2f%%)\" % (result.mean()*100, result.std()*100))\n",
    "history_dict=history.history\n",
    "history_dict.keys()\n",
    "acc_values=history_dict['accuracy']\n",
    "\n",
    "print(\"Result: %.2f%% (%.2f%%)\" % (numpy.mean(acc_values)*100, numpy.std(acc_values)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeoNv1Sj1SFF"
   },
   "source": [
    "**Rewriting the code by doing Model Subclassing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 93.69% (8.28%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "class My_model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(My_model,self).__init__()\n",
    "        self.dense1=Dense(60,activation='relu')\n",
    "        self.dense2=Dense(30,activation='relu')\n",
    "        self.dense3=Dense(1,activation='sigmoid')\n",
    "    def call(self,x):\n",
    "        x=self.dense1(x)\n",
    "        x=self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "model=My_model()\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "history=model.fit(X,encoded_Y, epochs=100,batch_size=5,verbose=False)\n",
    "\n",
    "history_dict=history.history\n",
    "history_dict.keys()\n",
    "acc_values=history_dict['accuracy']\n",
    "print(\"Result: %.2f%% (%.2f%%)\" % (numpy.mean(acc_values)*100, numpy.std(acc_values)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Z6KwJuD6pn1"
   },
   "source": [
    "**Rewriting the code without using scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(Dense(60,activation='relu',input_shape=(60,)))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n",
      "processing fold # 4\n",
      "processing fold # 5\n",
      "processing fold # 6\n",
      "processing fold # 7\n",
      "processing fold # 8\n",
      "processing fold # 9\n"
     ]
    }
   ],
   "source": [
    "k=10\n",
    "num_val_samples =len(X)//k\n",
    "all_score=[]\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = X[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = encoded_Y[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "#     print(val_data,val_targets)\n",
    "    train_data =numpy.concatenate([X[:i * num_val_samples],X[(i + 1) * num_val_samples:]],axis=0)\n",
    "    train_targets =numpy.concatenate([encoded_Y[:i * num_val_samples],encoded_Y[(i + 1) * num_val_samples:]],axis=0)\n",
    "    model = create_model()\n",
    "    model.fit(train_data, train_targets,epochs=50, batch_size=50,verbose=0)\n",
    "    loss,acc=model.evaluate(val_data,val_targets,verbose=0)\n",
    "#     val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "#     all_score.append(val_mae)\n",
    "# print(\"Result: %.2f%% (%.2f%%)\" % (numpy.mean(all_score)*100, numpy.std(all_score)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2FzypfqEnk1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3440415859222412\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(acc)\n",
    "model.save(\"simple_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SONAR(ROCK VS MINE) BINARY_CLASSIFICATION.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
